{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289e928-0dc4-40f6-9b5a-afc127b37d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb95c5f7-d52a-4e6e-a98c-80548981c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3db566-c892-469c-97e2-88158ed148c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = dim**0.5\n",
    "        self.g = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim=-1) * self.scale * self.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43437f-a688-4518-b1d5-09d316d9fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, activation: Callable, mult_bias=False):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "        self.mult_bias = nn.Parameter(torch.ones(dim_out)) if mult_bias else 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
    "        return x * self.act(gate) * self.mult_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a1b45-6602-4459-b752-c7e2ea4df844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2norm(t, groups=1):\n",
    "    B, H, T, E = t.size() # Batch, head size, block size, n_embd\n",
    "    t = t.view(B, H, T, groups, E//groups) # we split the last dimension into groups\n",
    "    t = F.normalize(t, dim=-1)\n",
    "    return t.view(B, H, T, E) # reassemble the input\n",
    "\n",
    "def pad_at_dim(t, pad, dim=-1, value=0.0):\n",
    "    dims_from_right = (-dim - 1) if dim < 0 else (t.ndim - dim - 1)\n",
    "    zeros = (0, 0) * dims_from_right\n",
    "    return F.pad(t, (*zeros, *pad), value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d85a9e-496e-4e33-af37-dc8f9680ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlibiPositionalBias(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Alibi positional bias. This will apply a negative bias to the sequence proportionally to its distance.\n",
    "    This means that close tokens will have more relevance than further ones.\n",
    "    \"\"\"\n",
    "    def __init__(self, heads, total_heads, **kwargs):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.total_heads = total_heads\n",
    "\n",
    "        slopes = Tensor(self._get_slopes(heads))\n",
    "        slopes = slopes.unsqueeze(1).unsqueeze(2)\n",
    "        self.register_buffer(\"slopes\", slopes, persistent=False)\n",
    "        self.register_buffer(\"bias\", None, persistent=False)\n",
    "\n",
    "    def get_bias(self, i, j, device):\n",
    "        i_arange = torch.arange(j - i, j, device=device)\n",
    "        j_arange = torch.arange(j, device=device)\n",
    "        bias = -torch.abs(\n",
    "            j_arange.unsqueeze(0).unsqueeze(0)\n",
    "            - i_arange.unsqueeze(0).unsqueeze(2)\n",
    "        )\n",
    "        return bias\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_slopes(heads):\n",
    "        \"\"\"\"\n",
    "        This function returns the slopes for the power of 2 of the heads. If the heads are not an exponent of 2 then we correct it.\n",
    "        \"\"\"\n",
    "        def get_slopes_power_of_2(n):\n",
    "            start = 2 ** (-(2 ** -(math.log2(n) - 3)))\n",
    "            ratio = start\n",
    "            return [start * ratio**i for i in range(n)]\n",
    "\n",
    "        if math.log2(heads).is_integer():\n",
    "            return get_slopes_power_of_2(heads)\n",
    "\n",
    "        closest_power_of_2 = 2 ** math.floor(math.log2(heads))\n",
    "        return sorted(\n",
    "            get_slopes_power_of_2(closest_power_of_2)\n",
    "            + get_slopes_power_of_2(2 * closest_power_of_2)[0::2][\n",
    "                : heads - closest_power_of_2\n",
    "            ], reverse=True\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.buffers()).device\n",
    "\n",
    "    def forward(self, i, j):\n",
    "        h, device = self.total_heads, self.device\n",
    "\n",
    "        if (\n",
    "            self.bias is not None\n",
    "            and self.bias.shape[-1] >= j\n",
    "            and self.bias.shape[-2] >= i\n",
    "        ):\n",
    "            return self.bias[..., :i, :j]\n",
    "\n",
    "        bias = self.get_bias(i, j, device)\n",
    "        bias = bias * self.slopes\n",
    "\n",
    "        num_heads_unalibied = h - bias.shape[0]\n",
    "        bias = pad_at_dim(bias, (0, num_heads_unalibied), dim=0)\n",
    "        self.register_buffer(\"bias\", bias, persistent=False)\n",
    "\n",
    "        return self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b08576-8029-4614-8eb3-f229fb1cf0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(t, freqs, scale=1):\n",
    "    seq_len = t.shape[-2]\n",
    "    freqs = freqs[-seq_len:, :]\n",
    "    return (t * freqs.cos() * scale) + (rotate_half(t) * freqs.sin() * scale)\n",
    "\n",
    "def rotate_half(x):\n",
    "    x = x.reshape(x.shape[:-1] + (2, -1))\n",
    "    x1, x2 = x.unbind(dim=-2)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def create_causal_mask(i, j, device):\n",
    "    return torch.ones((i, j), device=device, dtype=torch.bool).triu(j - i + 1)\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        use_xpos=True,\n",
    "        scale_base=512,\n",
    "        interpolation_factor=1.0,\n",
    "        base=10000,\n",
    "        alpha=1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/\n",
    "        base *= alpha ** (dim / (dim - 2))\n",
    "\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        assert interpolation_factor >= 1.0\n",
    "        self.interpolation_factor = interpolation_factor\n",
    "\n",
    "        if not use_xpos:\n",
    "            self.register_buffer(\"scale\", None)\n",
    "            return\n",
    "\n",
    "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "\n",
    "        self.scale_base = scale_base\n",
    "        self.register_buffer(\"scale\", scale)\n",
    "\n",
    "    def forward(self, seq_len, device):\n",
    "        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "        t = t / self.interpolation_factor\n",
    "        \n",
    "        freqs = t[:, None] * self.inv_freq[None, :]\n",
    "        freqs = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "        if self.scale is None:\n",
    "            return freqs, 1.0\n",
    "\n",
    "        power = (\n",
    "            torch.arange(seq_len, device=device) - (seq_len // 2)\n",
    "        ) / self.scale_base\n",
    "        scale = self.scale ** power.unsqueeze(1)\n",
    "        scale = torch.cat((scale, scale), dim=-1)\n",
    "\n",
    "        return freqs, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4be69-6c6f-433c-8a3f-33744f4d6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        self.kv_heads = config.kv_heads\n",
    "\n",
    "        q_dim = config.dim_head * config.n_head\n",
    "        k_dim = config.dim_head * config.kv_heads\n",
    "        v_dim = config.dim_head * config.kv_heads\n",
    "        out_dim = config.dim_head * config.n_head\n",
    "        \n",
    "        self.qk_norm_q_scale = nn.Parameter(torch.ones(config.dim_head))\n",
    "        self.qk_norm_k_scale = nn.Parameter(torch.ones(config.dim_head))\n",
    "        \n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.linear_enc = nn.Linear(config.n_embd*2, config.n_embd, bias=config.bias)\n",
    "        self.to_q = nn.Linear(config.n_embd, q_dim, bias=config.bias)\n",
    "        self.to_k = nn.Linear(config.n_embd, k_dim, bias=config.bias)\n",
    "        self.to_v = nn.Linear(config.n_embd, v_dim, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(out_dim, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.kv_head = config.kv_heads\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.mask = config.mask\n",
    "        self.dim_head = config.dim_head\n",
    "        self.qk_norm_scale = 10\n",
    "        assert hasattr(torch.nn.functional, 'scaled_dot_product_attention'), \"Flash Attention requires PyTorch >= 2.0\"\n",
    "        \n",
    "\n",
    "    def forward(self, x, encoded_data, rotary_pos_emb, pos_emb):\n",
    "        B, T, C = x.size() # batch size, block size, embedding dimensionality (dim_head)\n",
    "\n",
    "        q = self.to_q(x) # batch size, sequence length, dim_head*n_head\n",
    "        k = self.to_k(encoded_data) # batch size, sequence length, dim_head*kv_head\n",
    "        v = self.to_v(encoded_data) # batch size, sequence length, dim_head*kv_head\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, self.dim_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.kv_head, self.dim_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.kv_head, self.dim_head).transpose(1, 2)\n",
    "        # We arrange the heads -> batch_size, n head (or kv head), block size, dim_head\n",
    "        \n",
    "        q, k = map(l2norm, (q, k))\n",
    "        q = q * self.qk_norm_q_scale\n",
    "        k = k * self.qk_norm_k_scale\n",
    "        # We apply qk normalization on the dot products\n",
    "        \n",
    "        freqs, xpos_scale = rotary_pos_emb # block_size, dim_head/2\n",
    "        l = freqs.shape[-1]\n",
    "        \n",
    "        q_xpos_scale, k_xpos_scale = (\n",
    "            (xpos_scale, xpos_scale**-1.0)\n",
    "            if xpos_scale is not None\n",
    "            else (1.0, 1.0)\n",
    "        )\n",
    "\n",
    "        (ql, qr), (kl, kr), (vl, vr) = map(\n",
    "            lambda t: (t[..., :l], t[..., l:]), (q, k, v)\n",
    "        )\n",
    "        # We split the qkv values into the left and right parts on the n_embd dimension.\n",
    "        # batch, n_head, block_size, dim_head/2\n",
    "\n",
    "        ql, kl, vl = map(\n",
    "            lambda arg: apply_rotary_pos_emb(arg[0], freqs, arg[1]),\n",
    "            ((ql, q_xpos_scale), (kl, k_xpos_scale), (vl, k_xpos_scale)),\n",
    "        )\n",
    "        # We apply the rotary embeddings only to the left part\n",
    "        \n",
    "        q, k, v = map(\n",
    "            lambda t: torch.cat(t, dim=-1), ((ql, qr), (kl, kr), (vl, vr))\n",
    "        )\n",
    "        # We rebuild the qkv values into the old dimensions -> batch_size, n head (or kv), block size, dim_head\n",
    "        \n",
    "        attn_bias = pos_emb(T, T)\n",
    "        \n",
    "        k = k.repeat(1,2,1,1)\n",
    "        v = v.repeat(1,2,1,1)\n",
    "        # We repeat the dimensions needed to the kv heads\n",
    "        \n",
    "        default_scale = q.shape[-1] ** -0.5\n",
    "        q *= (default_scale / self.qk_norm_scale)\n",
    "        # We apply normalization again to the q value?\n",
    "            \n",
    "        attn_bias = attn_bias.unsqueeze(0).expand(B, self.n_head, -1, -1)\n",
    "        mask_value = -torch.finfo(q.dtype).max\n",
    "        causal_mask = create_causal_mask(\n",
    "            q.size(-2), k.size(-2), device=device\n",
    "        )\n",
    "        # We add the batch dimension to the attn bias\n",
    "\n",
    "        attn_bias = attn_bias.masked_fill(causal_mask, mask_value // 2)\n",
    "\n",
    "        # attend:  (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=attn_bias, dropout_p=self.dropout * self.training, is_causal=False)  # [B, n_head, block_size, dim_head]\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, self.dim_head * self.n_head) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f53b8-ee27-4ed3-8b1d-8919d6cc9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.glu     = GLU(config.n_embd, 4 * config.n_embd, nn.SiLU())\n",
    "        self.norm    = LayerNorm(4 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.glu(x)\n",
    "        x = self.c_proj(self.norm(x))\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cfc34b-7d05-46dc-83e7-9801a7f5d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.ln_2 = RMSNorm(config.n_embd)\n",
    "        self.cross_attn = CrossAttention(config)\n",
    "        self.ln_3 = RMSNorm(config.n_embd)\n",
    "        self.linear = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, encoded_x, rotary_pos_emb, pos_emb):\n",
    "        # We add x to each layer to skip connections.\n",
    "        x = x + self.cross_attn(self.ln_2(x), encoded_x, rotary_pos_emb, pos_emb)\n",
    "        x = x + self.mlp(self.ln_3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07026252-d242-4035-b2cb-f1a9585fc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecoderConfig:\n",
    "    block_size: int = 2046\n",
    "    vocab_size: int = 10172\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 320\n",
    "    dim_head: int = 64\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = False # True: bias in Linears and LayerNorms. False: a bit better and faster\n",
    "    mask: bool = False # Whether or not the attention is causal. (the future tokens get masked away)\n",
    "    kv_heads: int = 2\n",
    "    alibi_num_heads: int = 3\n",
    "    device: str = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abd9be-eae5-4066-a1d1-ef08de7b34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.base_std = 1/math.sqrt(config.n_embd) # The base std for initialization is calculated by taking the inverse of the sqrt of the embedding size\n",
    "        \n",
    "        self.decoder = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            rms = RMSNorm(config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            snorm = RMSNorm(config.n_embd),\n",
    "        ))\n",
    "        \n",
    "        rotary_emb_dim = config.dim_head // 2\n",
    "        self.rotary_pos_emb = RotaryEmbedding(\n",
    "            rotary_emb_dim,\n",
    "            scale_base=512,\n",
    "            interpolation_factor=1,\n",
    "            alpha=1,\n",
    "        )\n",
    "        self.pos_emb = AlibiPositionalBias(config.alibi_num_heads, config.n_head)\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # This applies weight tying. It is useful to tie the weights of the head that generates the logit tokens with the token embedding layer.\n",
    "        self.decoder.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=self.base_std/math.sqrt(2 * config.n_layer)) # Why 2? 2 operations on each Block\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.base_std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.base_std)\n",
    "\n",
    "    def forward(self, idx, imgs, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "\n",
    "        tok_emb = self.decoder.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        \n",
    "        x = self.decoder.drop(self.decoder.rms(tok_emb))\n",
    "        \n",
    "        rotary_pos_emb = self.rotary_pos_emb(\n",
    "            x.size(1), x.device\n",
    "        )\n",
    "        \n",
    "        for idx in range(self.config.n_layer):\n",
    "            x = self.decoder.h[idx](x, imgs, rotary_pos_emb, self.pos_emb)\n",
    "            x = self.decoder.snorm(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # during inference we only need to apply the head to the temporal dimension\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, imgs, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            x = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "\n",
    "            logits, _ = self(x, imgs)\n",
    "                        \n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "                \n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            sample = torch.multinomial(probs, 1)\n",
    "\n",
    "            idx = torch.cat((idx, sample), dim=-1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91ed47-77cf-4a1b-ab26-b404990de4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10172\n",
    "block_size = 2046\n",
    "n_layer = 4\n",
    "n_embd = 320\n",
    "dropout = 0.0\n",
    "n_head = 4\n",
    "\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "bias = False\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c915b32-8b05-4d2e-a67c-3a4abca2be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c48a89-b97e-40e0-baa6-09b9d910cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb71355-7bab-498c-92dd-192305dc3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=vocab_size, dropout=dropout, device=device) # init the model\n",
    "\n",
    "# This sets the matrix calculations precision to tensorfloat 32, which speeds up computation by a lot, with negligible cost for precision\n",
    "# Check https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html for more info\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575ff16-e9b7-4f54-bb40-162d6ce8bae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "def patch_img(x: Tensor, patches: int):\n",
    "    B, C, H, W = x.size()\n",
    "\n",
    "    # Step 1: Reshape to (b, c, h//patch_size, patch_size, w//patch_size, patch_size)\n",
    "    x_reshaped = x.view(B, C, H // patches, patches, W // patches, patches)\n",
    "\n",
    "    # Step 2: Permute to (b, h//patch_size, w//patch_size, patch_size, patch_size, c)\n",
    "    x_permuted = x_reshaped.permute(0, 2, 4, 3, 5, 1)\n",
    "\n",
    "    # Step 3: Reshape to (b, h//patch_size * w//patch_size, patch_size * patch_size * c)\n",
    "    x_final = x_permuted.contiguous().view(B, (H // patches) * (W // patches), patches * patches * C)\n",
    "    return x_final\n",
    "\n",
    "\n",
    "def threed_to_text_fixed(x: torch.Tensor, W1: Tensor, W2: Tensor):\n",
    "    \"\"\"\n",
    "    Transforms a patched 3d image into a text representation using a fixed transformation provided by W1 and W2.\n",
    "    x has to be patched.\n",
    "    W1 has to have dimensions -> (x.size(-1), n_embd)\n",
    "    W2 has to have dimensions -> (x.size(1), block_size)\n",
    "    \"\"\"\n",
    "    x = x @ W1\n",
    "    x = x.transpose(1, 2)\n",
    "    x = x @ W2\n",
    "    x = x.transpose(1, 2)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "class Fuyu(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        image_reshape_is_learnable: bool=False,\n",
    "        patches: int=16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patches = patches\n",
    "        self.fuyu = Transformer(dconf)\n",
    "        self.s_norm = LayerNorm(self.config.n_embd, bias=True)\n",
    "        self.image_reshape_is_learnable = image_reshape_is_learnable\n",
    "        \n",
    "        if not image_reshape_is_learnable:\n",
    "            # In case the image transformation does not need learnable parameters we will do it using fixed matrices\n",
    "            random_img = torch.randn(1,3,256,256)\n",
    "            random_img = patch_img(random_img, patches=self.patches)\n",
    "            _, S, D = random_img.size()\n",
    "            # Fixed transformation matrices.\n",
    "            self.W1 = torch.randn(D, config.n_embd, device=config.device)\n",
    "            self.W2 = torch.randn(S, config.block_size, device=config.device)\n",
    "        else:\n",
    "            # If the image reshaping needs to be learnable\n",
    "            self.threed_to_text = nn.Sequential(\n",
    "                nn.Linear(self.patches*self.patches*3, self.config.n_embd),\n",
    "                nn.Linear(256, self.config.block_size)\n",
    "            )\n",
    "        \n",
    "    def forward(\n",
    "        self, text: torch.Tensor, img: torch.Tensor=None, targets: torch.Tensor=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Text input shape: [batch, block_size, n_embd]\n",
    "        img input shape: [batch, channels, height, width]\n",
    "\n",
    "        Output shape: [batch, 1, vocab_size]\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # If image is provided, concat it with the text\n",
    "            if img is not None:\n",
    "                # Patch the image\n",
    "                img = patch_img(img, patches=self.patches)\n",
    "                if self.image_reshape_is_learnable:\n",
    "                    img = self.threed_to_text[0](img)\n",
    "                    img = img.transpose(1, 2)\n",
    "                    img = self.threed_to_text[1](img)\n",
    "                    img = img.transpose(1, 2)\n",
    "                else:\n",
    "                    img = threed_to_text_fixed(img, self.W1, self.W2)\n",
    "                img = self.s_norm(img)\n",
    "            return self.fuyu(text, img, targets)\n",
    "        except Exception as e:\n",
    "            print(\"Failed in forward method: \", e)\n",
    "            raise\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, text, img, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        if img is not None:\n",
    "            # Patch the image\n",
    "            img = patch_img(img, patches=self.patches)\n",
    "            if self.image_reshape_is_learnable:\n",
    "                img = self.threed_to_text[0](img)\n",
    "                img = img.transpose(1, 2)\n",
    "                img = self.threed_to_text[1](img)\n",
    "                img = img.transpose(1, 2)\n",
    "            else:\n",
    "                img = threed_to_text_fixed(img, self.W1, self.W2)\n",
    "            img = self.s_norm(img)\n",
    "        return self.fuyu.generate(text, img, max_new_tokens, temperature, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecc174-3b47-45a9-9269-30e501ea4d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dconf = DecoderConfig(**model_args)\n",
    "model = Fuyu(\n",
    "    dconf,\n",
    "    patches=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7269df-0397-4472-b7be-2bc86834c86d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e396de3-1b47-4bc3-9f87-1e27434f9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text shape: [batch, block_size]\n",
    "text = torch.randint(0, vocab_size, (1, block_size), device=device)\n",
    "\n",
    "# Img shape: [batch, channels, height, width]\n",
    "img = torch.randn(1, 3, 256, 256, device=device)\n",
    "\n",
    "# Random targets to test the loss function\n",
    "targets = torch.randint(0, vocab_size, (1, block_size), device=device)\n",
    "\n",
    "# Apply model to text and img\n",
    "y = model(text, img, targets=None)\n",
    "\n",
    "# Output shape: [batch, block_size, vocab_size] if targets are provided. [batch, 1, vocab_size] if targets are not provided\n",
    "print(y, y[0].size(), y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2e1bc-26ec-4c63-9547-ea8fb6ffc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample = model.generate(text, img, 30, temperature=1, top_k=200)\n",
    "sample.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
